{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11b6934c-f443-423d-b9a3-1edd7b3233a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Importing all the neccessary packages ###\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy\n",
    "from matplotlib.offsetbox import AnchoredText\n",
    "import cartopy.feature as cfeature\n",
    "import scipy.fft as sf\n",
    "from scipy import signal\n",
    "from scipy.stats import circmean\n",
    "from scipy import optimize\n",
    "import xarray.ufuncs as xu\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import time\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a992297",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### Allows us to use dask to speed up some calculations ###\n",
    "from dask.distributed import Client\n",
    "client = Client()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96371339-e2f6-43c1-8490-8526e25c4688",
   "metadata": {},
   "outputs": [],
   "source": [
    "### This is the form of fourier series both matlab and python utilize ###\n",
    "\n",
    "from IPython.display import SVG, display\n",
    "SVG(url='https://wikimedia.org/api/rest_v1/media/math/render/svg/908d750bbbc3c640ef8103852f9843d283f06b65')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78264c95-8654-48da-9491-85feb0b7ed0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def curve_2harmonic(x, a0, a1, b1):\n",
    "    output = (a0 +\n",
    "    a1*np.cos(x/len(x)*2*np.pi) + b1*np.sin(x/len(x)*2*np.pi))\n",
    "    return output\n",
    "\n",
    "def fit_curve2(xdata, ydata):\n",
    "    guess = [np.mean(ydata), 0, 0]\n",
    "    params, params_covariance = optimize.curve_fit(curve_2harmonic, xdata, ydata, guess)\n",
    "    return params, params_covariance\n",
    "\n",
    "def fourier1(tseries):\n",
    "    mtot=len(tseries)\n",
    "    time=np.arange(1,mtot+1,1.)\n",
    "    params, _ = fit_curve2(time,tseries)\n",
    "    return np.argmin(curve_2harmonic(time,*params))\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513f2727-20f7-4e0e-9dc6-b8febf01d653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seasonality(tseries):\n",
    "    mtot = len(tseries)\n",
    "    time=np.arange(1,mtot+1,1.)\n",
    "    params, _ = fit_curve2(time,tseries)\n",
    "    return (params[2]/params[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cdc2e8-de6d-4b10-849c-67bd4a0e266b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ac2e8f-34f6-4c24-ad7f-97a576430707",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Directory For Narwhal ###\n",
    "data_dir = '/data/deluge/reanalysis/REANALYSIS/ERA5/2D/daily/precip/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf65dbd-b807-4708-9711-db862a4895dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Bombardi et al., 2019 has put the code on github under the MIT license, which allows us to utilize their code in any way we see fit. \n",
    "### We will of course credit Bombardi et al., 2019 in the eventual manuscript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d712268-64bb-4430-9290-10321a08db4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Opening all of the ERA5 daily precipitation data. The 'time':-1 section loads the data such that each 'chunk' has the entire time series, but is only 100x100 lat, lon.\n",
    "ds = xr.open_mfdataset(data_dir+'*.nc',parallel=True, chunks={'latitude': 25, 'longitude': 25, 'time': -1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbab4a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "### subsetting the data so we grab only north america.\n",
    "subset=ds.sel(latitude=slice(50,30), longitude=slice((360.0-125),(360.0-65.0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7c4499-d80a-4662-8247-b51c5ee82f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Showing the advantage of using Dask to perform operations ###\n",
    "### Time without dask:  319 seconds.  \n",
    "### Time with dask: 155 seconds.\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "\n",
    "ds_subset = subset.load()\n",
    "end = time.time()\n",
    "print(end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259418f6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ds_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb61fe6-b69e-4ce9-9dea-3bc5db08498c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Changing data to mm and removing leap days ###\n",
    "ds_subset['precip'].values = ds_subset['precip'].values*1000.0\n",
    "precip = ds_subset['precip']\n",
    "precip = precip.sel(time=~((precip.time.dt.month == 2) & (precip.time.dt.day == 29)))\n",
    "\n",
    "### Calculating the mean precipitation for each grid point\n",
    "annual_mean_precip = precip.mean(dim='time')\n",
    "\n",
    "### Calculating the mean annual cycle ###\n",
    "annual_precip_cycle= precip.groupby('time.dayofyear').mean(dim='time')\n",
    "\n",
    "annual_precip_cycle_var = precip.groupby('time.dayofyear').std(dim='time')\n",
    "\n",
    "### Create Daily precip from hourly ###\n",
    "##daily_precip = precip.resample(time='1D').sum()\n",
    "\n",
    "### Create The annual daily precip anomalies ###\n",
    "annual_means = precip.groupby('time.year').mean(dim='time')\n",
    "\n",
    "anomalies = precip.groupby('time.year') - annual_means\n",
    "#anomalies = anomalies_withleap.sel(time=~((anomalies_withleap.time.dt.month == 2) & (anomalies_withleap.time.dt.day == 29)))\n",
    "\n",
    "climDailyMeanAnomaly = annual_precip_cycle - annual_mean_precip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb40f61-684b-4045-862f-d22dd0487b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code acquired from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bc4efe-0f96-4266-aec7-dfce598cbcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Funtion that calculates the Fourier coefficients and the explained variance of the Nth\n",
    "first harmonics of a time series\n",
    "Input:\n",
    "   tseries: input time series\n",
    "   nmodes : number of harmonics to retain (N)\n",
    "   coefa  : Array with N (or 'nmodes') elements\n",
    "   coefb  : Array with N (or 'nmodes') elements\n",
    "   hvar   : Array with N (or 'nmodes') elements\n",
    "   missval: Falg value for missing data\n",
    "Output:\n",
    "   coefa: Array of A coefficients of the Nth first harmonics\n",
    "   coefb: Array of B coefficients of the Nth first harmonics\n",
    "   hvar : Array of explained variance of the Nth first harmonics\n",
    "\"\"\"\n",
    "\n",
    "### This function returns the minimum of the 1st harmonic which will serve as the start date for onset calculation.\n",
    "\n",
    "def Harmonics(tseries, nmodes=2,missval=np.nan):\n",
    "    tot = 366 ### Dealing with only 1 year of data here\n",
    "    mtot=len(tseries)\n",
    "    time=np.arange(1,mtot+1,1.)\n",
    "    newdim=len(tseries)  # removing missing data\n",
    "    harmonic1 =np.zeros((tot))\n",
    "    tdata=tseries\n",
    "    svar=np.sum((tdata[:]-np.mean(tdata))**2)//(newdim-1)\n",
    "    nm=nmodes\n",
    "    if 2*nm > newdim:\n",
    "        nm=newdim/2\n",
    "    coefa=np.zeros((nm))\n",
    "    coefb=np.zeros((nm))\n",
    "    hvar=np.zeros((nm))\n",
    "    harmonic1[:] = np.mean(tseries)\n",
    "    for tt in range(0,nm):\n",
    "        Ak=np.sum(tdata[:]*xu.cos(2.*np.pi*(tt+1)*time[:]/float(newdim)))\n",
    "        Bk=np.sum(tdata[:]*xu.sin(2.*np.pi*(tt+1)*time[:]/float(newdim)))\n",
    "        coefa[tt]=Ak*2./float(newdim)\n",
    "        coefb[tt]=Bk*2./float(newdim)\n",
    "        hvar[tt]=newdim*(coefa[tt]**2+coefb[tt]**2)/(2.*(newdim-1)*svar)\n",
    "        harmonic1=harmonic1+coefa[tt]*np.cos(2.*np.pi*time[:]/float(tot))+coefb[tt]*np.sin(2.*np.pi*time[:]/float(tot))\n",
    "        \n",
    "    # if hvar[1] >= hvar[0]:\n",
    "    #     return np.nan\n",
    "    # elif hvar[2] >= hvar[0]:\n",
    "    #     return np.nan\n",
    "    # else:\n",
    "    return harmonic1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf3fb13-5581-4dac-b697-fa2113427965",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ceb29d-7d93-4d63-92e1-7897ef19bfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use dask to run the Harmonics finction over the data ### \n",
    "### Slightly faster than a loop but still slow ###\n",
    "start_wet2 = xr.apply_ufunc(\n",
    "fourier1,\n",
    "annual_precip_cycle.load(),\n",
    "input_core_dims=[[\"dayofyear\"]],\n",
    "exclude_dims=set([\"dayofyear\"]),\n",
    "vectorize=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e7d98c-49ef-410f-9a7d-b9c40215c547",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_wet2.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fee7c71",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "array_jday = anomalies.time.dt.dayofyear\n",
    "input_data = anomalies.assign_coords(jday=(\"time\",array_jday.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cd8fb6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "start_wet2.to_netcdf('algorithm_start.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7651a87",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### Used for testing individual areas ###\n",
    "lat = 43.5\n",
    "lon = 270.5\n",
    "data = input_data.sel(latitude=lat, longitude=lon).values\n",
    "data_time = input_data.sel(latitude=lat, longitude=lon).time.values\n",
    "data_jday = input_data.sel(latitude=lat, longitude=lon).jday.values\n",
    "\n",
    "start_test = start_wet2.sel(latitude=lat, longitude=lon).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ff9c3d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#========================================================================\n",
    "#  Subroutine that calculates the beginning date of the rainy season\n",
    "# for a time series of precipitation\n",
    "# nyrs   --> integer for the number of years in the input dataset\n",
    "# tot    --> total number of points for one year of data (365)\n",
    "# mtot   --> total number of points in the whole precipitation dataset\n",
    "# jday   --> an array of Julian days\n",
    "# day    --> an array of days\n",
    "# month  --> an array of months\n",
    "# year   --> an array of years\n",
    "# jstart --> Julian day of the climatological date when the calculation\n",
    "#            should start\n",
    "# precip --> a time series of precipitation anomalies (against mean annual daily)\n",
    "# npass  --> integer for the number of \"passes\" for the smoothing of the\n",
    "#            time series of accumulated precipitation anomalies\n",
    "#========================================================================#def rainyseason_onset(nyrs,ytot,jday,day,month,year,jstart,precip,sjday,sday,smonth,syear,curve):\n",
    "#def rainyseason_onset(anomaly_ds,start_wet):\n",
    "#\n",
    "### From the apply_ufunc for testing ###\n",
    "data = input_data.sel(latitude=lat, longitude=lon).values\n",
    "data_time = input_data.sel(latitude=lat, longitude=lon).time.values\n",
    "data_jday = input_data.sel(latitude=lat, longitude=lon).jday.values\n",
    "start_test = start_wet2.sel(latitude=lat, longitude=lon).values\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70df7444-dc18-424e-b6fa-38cb90a66b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data.sel(latitude=lat, longitude=lon, time=slice('1959','1960')).plot(figsize=(28,10))\n",
    "input_data.sel(latitude=lat, longitude=lon, time=slice('1959','1960')).cumsum().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c69a7ca-ab08-4a70-9767-ffcc03567e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_onset = onset_LM01(data,data_time,start_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0722f276-ff9c-44f0-be16-2ff9f3f39428",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_onset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c78df58-7c44-4087-949f-a760b3c8742a",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_wet2.sel(latitude=lat,longitude=lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413fddec-b556-436a-9e6f-c3ee8ee26e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_demise[1:]- test_onset[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a37ae47-5c33-472c-b300-b80d62701c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_onset[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f68f465-a490-4c0e-a621-82777c0efac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_demise[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458da145-5034-4879-829c-03c6ad3d6dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=0\n",
    "input_data.sel(latitude=lat, longitude=lon, time=slice(test_onset[:-1][n]-50,test_demise[1:][n]+50)).plot(figsize=(28,10))\n",
    "plt.axvline(test_onset[:-1][n], color='green')\n",
    "plt.axvline(test_demise[1:][n],color='brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c2b42b-e3cb-4a84-9238-e86d33add1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def onset_dunning(data, data_time, start_test):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f63185a-ac2d-4fc1-be3b-eecb09098011",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_time[0] - (data_time[0] - 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac2866c-1a41-4477-91ad-7d940f44ad18",
   "metadata": {},
   "outputs": [],
   "source": [
    "0 <= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1987a855-ccde-4cc2-af3c-f22a32570fcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def onset_LM01(data, time, startWet):\n",
    "    ### Requires anomaly data ###\n",
    "    dataLength = len(data)\n",
    "    if (startWet <= 0):\n",
    "        startWet=1\n",
    "    data_time = pd.DatetimeIndex(time)\n",
    "    \n",
    "    ### dayofyear is 1 indexed ###\n",
    "    startDOY= np.where(data_time.dayofyear == startWet)[0]\n",
    "    \n",
    "    nYears = len(np.unique(data_time.year))\n",
    "    \n",
    "    onsetDOY=np.empty((nYears))\n",
    "    onsetDOY[:] = np.nan\n",
    "    onsetDate=np.empty((nYears),dtype='datetime64[D]')\n",
    "    onsetDate[:] = 'nat'\n",
    "    #print(data_time[::-1])\n",
    "    ### looping through start dates ###\n",
    "    for i, start in enumerate(startDOY):\n",
    "        \n",
    "        ### Make sure we dont exceed our data ###\n",
    "        if start < dataLength:\n",
    "            analysisBegin = start\n",
    "            analysisEnd = start + int(180) ### end of analysis is 180 days later\n",
    "            ### Make sure that we have enough data to compute ###\n",
    "            if (analysisEnd > dataLength - 180):\n",
    "                ### arrays initialized as Nans ###\n",
    "                pass\n",
    "            analysisPeriod = data_time[analysisBegin:analysisEnd]\n",
    "            sumSeries = np.cumsum(data[analysisBegin:analysisEnd])\n",
    "            \n",
    "            onset = np.argmin(sumSeries)\n",
    "            #print(onset)\n",
    "            onsetDOY[i] = onset\n",
    "            onsetDate[i] = analysisPeriod[onset]\n",
    "            if (analysisBegin < onset < analysisEnd):\n",
    "                #print(onset)\n",
    "                onsetDOY[i] = onset\n",
    "                onsetDate[i] = analysisPeriod[onset]\n",
    "            #if (i==2):\n",
    "                #break\n",
    "    return onsetDate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0b85b4-18d5-4d67-8d79-0c8a118413c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def demise_LM01(data, time, startWet):\n",
    "    ### Requires anomaly data ###\n",
    "    dataLength = len(data)\n",
    "    data_time = pd.DatetimeIndex(time)\n",
    "    ### dayofyear is 1 indexed.\n",
    "    if (startWet <= 0):\n",
    "        startWet=1\n",
    "    \n",
    "    startDOY= np.where(data_time.dayofyear == startWet)[0]\n",
    "    nYears = len(np.unique(data_time.year))\n",
    "    \n",
    "    demiseDOY=np.empty((nYears))\n",
    "    demiseDOY[:] = np.nan\n",
    "    demiseDate=np.empty((nYears),dtype='datetime64[D]')\n",
    "    demiseDate[:] = 'nat'\n",
    "    #print(startWet)\n",
    "    ### looping through start dates ###\n",
    "    for i, start in enumerate(startDOY):\n",
    "        analysisBegin = start\n",
    "        analysisEnd = start - int(180)\n",
    "        ### Make sure we dont exceed our data ###\n",
    "        if analysisBegin < dataLength and analysisEnd > 0:\n",
    "             ### end of analysis is 180 days later\n",
    "            ### Make sure that we have enough data to compute ###\n",
    "            analysisPeriod = data_time[analysisBegin:analysisEnd:-1]\n",
    "            ### Cumulative Sum ###\n",
    "            sumSeries = np.cumsum(data[analysisBegin:analysisEnd:-1])\n",
    "            \n",
    "            \n",
    "            demise = np.argmin(sumSeries)\n",
    "            #print(demise)\n",
    "            #print(analysisPeriod[demise])\n",
    "            #print(onset)\n",
    "            demiseDOY[i] = demise\n",
    "            demiseDate[i] = analysisPeriod[demise]\n",
    "            if (analysisBegin > demise > analysisEnd):\n",
    "                #print(onset)\n",
    "                demiseDOY[i] = demise\n",
    "                demiseDate[i] = analysisPeriod[demise]\n",
    "            #if (i==2):\n",
    "                #break\n",
    "    return demiseDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46704e57",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def onset_bombardi(data, data_time, startWet):\n",
    "    #print(f\"data: {data.shape} | time: {data_time.shape} | start_test: {start_test.shape}\")\n",
    "    ### Precalculations ###\n",
    "    if (startWet <= 0):\n",
    "        startWet=1\n",
    "    sseries=np.zeros((int(366/2)))\n",
    "    dataLength = len(data)\n",
    "    data_time = pd.DatetimeIndex(data_time)\n",
    "    startDOY= np.where(data_time.dayofyear == startWet)[0]\n",
    "    #print(len(np.unique(startDOY)))\n",
    "    nyrs = len(np.unique(data_time.year))\n",
    "    ytot=365\n",
    "    ### Data Structures to hold results ###\n",
    "    sjday=np.empty((nyrs))\n",
    "    sjday[:] = np.nan\n",
    "    sdate=np.empty((nyrs),dtype='datetime64[D]')\n",
    "    sdate[:] = 'nat'\n",
    "    #smonth=np.zeros((nyrs))\n",
    "    #syear=np.zeros((nyrs))\n",
    "    ### Run through entire time series for one grid point ###\n",
    "    yt = -1\n",
    "    for i, start in enumerate(startDOY):\n",
    "        ### Loop through the list of start days ###\n",
    "        if start < (dataLength):         # -5 to avoid calcualtion with short time series for last year\n",
    "            \n",
    "            beg = start\n",
    "            end = beg+int(365/2)\n",
    "            if end <= dataLength-1:  # it is not the last year\n",
    "                end2=int(ytot/2)\n",
    "            if end > dataLength-1:\n",
    "                end=dataLength-1\n",
    "                end2=end-beg\n",
    "            sseries[:]=0\n",
    "            sseries[0:end2]=np.cumsum(data[beg:beg+end2])\n",
    "            #curve[yt,:]=sseries[:]\n",
    "            #-------------------------------------------------------------------------\n",
    "            # Calculating onset and demise of the rainy season\n",
    "            #-------------------------------------------------------------------------\n",
    "            beg=0\n",
    "            try:\n",
    "                ons=np.where(sseries[0:end2] == sseries[0:end2].min())\n",
    "            except ValueError:\n",
    "                pass\n",
    "                #print(beg)\n",
    "            if len(ons[0]) > 0:\n",
    "                \n",
    "                beg=ons[0][0]+start+1\n",
    "                #print(beg)\n",
    "            if beg > 0 and beg < end:\n",
    "\n",
    "                sjday[yt]= data_time[beg].dayofyear\n",
    "                sdate[yt]= data_time[beg]\n",
    "                #smonth[yt]= data_time[beg].month\n",
    "                #syear[yt]= data_time[beg].day\n",
    "    return sdate\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#========================================================================\n",
    "#                             End of subroutine\n",
    "#========================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8ec263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demise_calculation(data, data_time, startWet):\n",
    " ### Reverse Date for Demise Calculation\n",
    "    data = data[::-1]\n",
    "    data_time = data_time[::-1]\n",
    "    ### Precalculations ###\n",
    "    if (startWet <= 0):\n",
    "        startWet=1\n",
    "    sseries=np.zeros((int(366/2)))\n",
    "    dataLength= len(data)\n",
    "    yt=-1\n",
    "    data_time = pd.DatetimeIndex(data_time)\n",
    "    start_list = np.where(data_time.dayofyear == startWet)[0]\n",
    "    #print(start_test)\n",
    "    nyrs = len(np.unique(data_time.year))\n",
    "    ytot=365\n",
    "    ### Data Structures to hold results ###\n",
    "    sjday=np.empty((nyrs))\n",
    "    sjday[:] = np.nan\n",
    "    sdate=np.empty((nyrs),dtype='datetime64[D]')\n",
    "    sdate[:] = 'nat'\n",
    "    #smonth=np.zeros((nyrs))\n",
    "    #syear=np.zeros((nyrs))\n",
    "    ### Run through entire time series for one grid point ###\n",
    "    for start in start_list:\n",
    "        #print(start)\n",
    "        #    for tt in range(0,dataLength-5): # -5 to avoid calcualtion with short time series for last year\n",
    "        #------------------------------------------------------------------------\n",
    "        # Starting the calculation of accumulated anomalies in the rainy season\n",
    "        #------------------------------------------------------------------------                 !\n",
    "        #        if jday[tt] == jstart:\n",
    "        if start < (dataLength):         # -5 to avoid calcualtion with short time series for last year\n",
    "            yt=yt+1\n",
    "            beg= start\n",
    "            end = beg+int(365/2)\n",
    "            if end <= dataLength-1:  # it is not the last year\n",
    "                end2=int(ytot/2)\n",
    "            if end > dataLength-1:\n",
    "                end=dataLength-1\n",
    "                end2=end-beg\n",
    "            sseries[:]=0\n",
    "            sseries[0:end2]=np.cumsum(data[beg:beg+end2])\n",
    "            #curve[yt,:]=sseries[:]\n",
    "            #-------------------------------------------------------------------------\n",
    "            # Calculating onset and demise of the rainy season\n",
    "            #-------------------------------------------------------------------------\n",
    "            beg=0\n",
    "            try:\n",
    "                ons=np.where(sseries[0:end2] == sseries[0:end2].min())\n",
    "            except ValueError:\n",
    "                #print(beg)\n",
    "                pass\n",
    "            if len(ons[0]) > 0:\n",
    "                beg=ons[0][0]+start+1\n",
    "                #print(beg)\n",
    "            if beg > 0 and beg < end:\n",
    "\n",
    "                sjday[yt]= data_time[beg].dayofyear\n",
    "                sdate[yt]= data_time[beg]\n",
    "                #smonth[yt]= data_time[beg].month\n",
    "                #syear[yt]= data_time[beg].day\n",
    "    return sdate[::-1]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467a40f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 500 seconds ###\n",
    "\n",
    "### 423 without Dask... Need more optimization... ###\n",
    "start = time.time()\n",
    "\"the code you want to test stays here\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "demise_LM01_test = xr.apply_ufunc(\n",
    "    demise_LM01,\n",
    "    anomalies,\n",
    "    anomalies.time,\n",
    "    start_wet2,\n",
    "    input_core_dims=[[\"time\"],[\"time\"],[]],\n",
    "    exclude_dims=set([\"time\"]),\n",
    "    output_core_dims=[[\"year\"]],\n",
    "    vectorize=True,\n",
    "    dask = 'parallelized',\n",
    "    #output_dtypes = 'datetime64[D]',\n",
    "    #output_sizes={\"data_jday\": 71},\n",
    ")\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683193ef",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### 500 Seconds ###\n",
    "start = time.time()\n",
    "\"the code you want to test stays here\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "onset_LM01_test = xr.apply_ufunc(\n",
    "    onset_LM01,\n",
    "    anomalies,\n",
    "    anomalies.time,\n",
    "    start_wet2,\n",
    "    input_core_dims=[[\"time\"],[\"time\"],[]],\n",
    "    exclude_dims=set([\"time\"]),\n",
    "    output_core_dims=[[\"year\"]],\n",
    "    vectorize=True,\n",
    "    dask = 'parallelized',\n",
    "    #output_dtypes = 'datetime64[D]',\n",
    "    #output_sizes={\"data_jday\": 71},\n",
    ")\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b36de51-4a93-413a-81f8-7d727b82556f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 500 Seconds ###\n",
    "start = time.time()\n",
    "\"the code you want to test stays here\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "onset_bombardi_test = xr.apply_ufunc(\n",
    "    onset_bombardi,\n",
    "    anomalies,\n",
    "    anomalies.time,\n",
    "    start_wet2,\n",
    "    input_core_dims=[[\"time\"],[\"time\"],[]],\n",
    "    exclude_dims=set([\"time\"]),\n",
    "    output_core_dims=[[\"year\"]],\n",
    "    vectorize=True,\n",
    "    dask = 'parallelized',\n",
    "    #output_dtypes = 'datetime64[D]',\n",
    "    #output_sizes={\"data_jday\": 71},\n",
    ")\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af0fee4-8812-4667-8ae1-33783db87c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 500 Seconds ###\n",
    "start = time.time()\n",
    "\"the code you want to test stays here\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "demise_bombardi_test = xr.apply_ufunc(\n",
    "    demise_calculation,\n",
    "    anomalies,\n",
    "    anomalies.time,\n",
    "    start_wet2,\n",
    "    input_core_dims=[[\"time\"],[\"time\"],[]],\n",
    "    exclude_dims=set([\"time\"]),\n",
    "    output_core_dims=[[\"year\"]],\n",
    "    vectorize=True,\n",
    "    dask = 'parallelized',\n",
    "    #output_dtypes = 'datetime64[D]',\n",
    "    #output_sizes={\"data_jday\": 71},\n",
    ")\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e085f4c3-b32d-4427-b535-d7f67cfe4e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "onset_data = onset_LM01_test\n",
    "demise_data = demise_LM01_test\n",
    "onset_data.name = 'onset_date'\n",
    "demise_data.name = 'demise_date'\n",
    "onset_data.coords['year'] = pd.date_range(\"1951\", periods=70, freq='YS')\n",
    "demise_data.coords['year'] = pd.date_range(\"1951\", periods=70, freq='YS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75aae292-1f09-4b08-ad9d-a3a2fcccace3",
   "metadata": {},
   "outputs": [],
   "source": [
    "onset_data.isel(latitude = 20, longitude = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68451f5-cdff-4f15-af0f-86cb3ca31a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_onset = xr.merge([onset_data,demise_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab847541-6747-455f-a3a0-400137b11800",
   "metadata": {},
   "outputs": [],
   "source": [
    "onset_LM01_test.isel(latitude = 20, longitude = 30)[:-1] - demise_LM01_test.isel(latitude = 20, longitude = 30)[1:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea44fa97-b120-4d5c-8a17-f930ac279029",
   "metadata": {},
   "outputs": [],
   "source": [
    "onset_LM01_test = onset_LM01_test.isel(year=slice(0,70))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dc0b68-d670-4af5-b117-5ffa27a25000",
   "metadata": {},
   "outputs": [],
   "source": [
    "demise_LM01_test = demise_LM01_test.isel(year=slice(1,71))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac3dc2a-30c8-4d50-8bb9-4100164c924c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_onset['demise_doy'] = test_onset['demise_date'].dt.dayofyear\n",
    "test_onset['onset_doy'] = test_onset['onset_date'].dt.dayofyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885f640d-d9d7-432d-a11f-639130ad73cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_onset.to_netcdf('OnsetDemise_ERA5.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9958e1-01a5-4d02-9f70-7de3ed7d37ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_onset['onset_doy'].sel(year='2011').plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89e61ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "demise.to_netcdf('wetseason.demise.era5.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f87ac86-e958-45ec-8161-2d03311eac95",
   "metadata": {},
   "outputs": [],
   "source": [
    "onset.to_netcdf('wetseason.onset.era5.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc6d2e7-87ae-442c-984d-99f7b654d63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "(test_mean - demise['demise_date'].mean(dim='year')).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e02614",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Need to reverse the output array ###\n",
    "\n",
    "demise['demise_date'].mean(dim='year').plot(figsize=(13,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5980b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test6['onset_date'].mean(dim='data_jday').plot(figsize=(13,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d5b186-0f75-4e2a-aa03-ffd3d49eac59",
   "metadata": {},
   "outputs": [],
   "source": [
    "states_provinces = cfeature.NaturalEarthFeature(\n",
    "    category='cultural',\n",
    "    name='admin_1_states_provinces_lines',\n",
    "    scale='10m',\n",
    "    facecolor='none')\n",
    "map_proj = ccrs.LambertConformal(central_longitude=-95, central_latitude=45)\n",
    "#cmap = mpl.cm.RdBu_r\n",
    "\n",
    "\n",
    "f, ax1 = plt.subplots(1, 1, figsize=(10, 13), dpi=600, subplot_kw={'projection': map_proj})\n",
    "p = onset_data.dt.dayofyear.isel(year=50).plot.pcolormesh(ax=ax1,transform=ccrs.PlateCarree(), add_colorbar=False, cmap='viridis')\n",
    "\n",
    "\n",
    "### Setting 1st plot parameters ###\n",
    "ax1.coastlines(color='grey')\n",
    "ax1.add_feature(cartopy.feature.BORDERS, color='black')\n",
    "ax1.add_feature(cfeature.STATES, edgecolor='black')\n",
    "#ax1.set_xticks(np.arange(-180,181, 40))\n",
    "#ax1.set_yticks(np.arange(-90,91,15))\n",
    "ax1.set_xlabel('Longitude')\n",
    "ax1.set_ylabel('Latitude')\n",
    "at = AnchoredText(\"a\",\n",
    "                      loc='upper left', prop=dict(size=8), frameon=True,)\n",
    "at.patch.set_boxstyle(\"round,pad=0.,rounding_size=0.2\")\n",
    "ax1.add_artist(at)\n",
    "divider = make_axes_locatable(ax1)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05, axes_class=plt.Axes)\n",
    "plt.colorbar(p, cax=cax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f64d96",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
